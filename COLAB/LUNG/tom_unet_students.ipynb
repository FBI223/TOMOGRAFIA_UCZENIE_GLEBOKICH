{
  "cells": [
    {
      "metadata": {
        "id": "44be13338e7e0bce"
      },
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/FBI223/TOMOGRAFIA_UCZENIE_GLEBOKICH/blob/main/COLAB/LUNG/tom_unet_students.ipynb)\n"
      ],
      "id": "44be13338e7e0bce"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install celluloid\n",
        "!pip install -U imgaug imageio\n",
        "!pip install pytorch-lightning torch torchvision torchmetrics nibabel tqdm opencv-python\n",
        "!pip install onedrivedownloader\n"
      ],
      "metadata": {
        "id": "bek-zFumTQir"
      },
      "id": "bek-zFumTQir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onedrivedownloader import download\n",
        "\n",
        "url = \"https://ujchmura-my.sharepoint.com/:u:/g/personal/marcin_sztukowski_student_uj_edu_pl/EURfIwcEpYVKv88gopAwaeoBbNeeBOqWz3z4IkzUaI5uqg?e=aaQSj9\"\n",
        "download(url, filename=\"Task06_Lung.tar\", unzip=False)\n",
        "!tar -xvf /content/Task06_Lung.tar -C /content/"
      ],
      "metadata": {
        "id": "I-BmEJs5cgHR"
      },
      "id": "I-BmEJs5cgHR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "color-nashville",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:52.865015Z",
          "start_time": "2025-11-06T21:24:52.504570Z"
        },
        "id": "color-nashville"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from celluloid import Camera\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liberal-standard",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:52.871872Z",
          "start_time": "2025-11-06T21:24:52.869102Z"
        },
        "id": "liberal-standard"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Ustal folder projektu wzglƒôdem lokalizacji notebooka\n",
        "project_root = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
        "\n",
        "# ≈öcie≈ºki wzglƒôdne\n",
        "root = project_root / \"Task06_Lung\" / \"imagesTr\"\n",
        "label = project_root / \"Task06_Lung\" / \"labelsTr\"\n",
        "\n",
        "print(\"Images path:\", root)\n",
        "print(\"Labels path:\", label)\n",
        "print(\"Images path exists:\", root.exists())\n",
        "print(\"Labels path exists:\", label.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helpful-significance",
      "metadata": {
        "id": "helpful-significance"
      },
      "source": [
        "**Task: Load a sample NIfTI and its corresponding label mask**<br />\n",
        "Hint: You might want to define a helper function to make your life easier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "effective-programmer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:52.878461Z",
          "start_time": "2025-11-06T21:24:52.876551Z"
        },
        "id": "effective-programmer"
      },
      "outputs": [],
      "source": [
        "def change_img_to_label_path(path):\n",
        "    \"\"\"\n",
        "    Replaces imagesTr with labelsTr\n",
        "    \"\"\"\n",
        "    parts = list(path.parts)  # get all directories whithin the path\n",
        "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"  # Replace imagesTr with labelsTr\n",
        "    return Path(*parts)  # Combine list back into a Path object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-jacket",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:52.896832Z",
          "start_time": "2025-11-06T21:24:52.893933Z"
        },
        "id": "adaptive-jacket"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"Task06_Lung\")\n",
        "\n",
        "# pobierz ≈õcie≈ºki do obraz√≥w i masek\n",
        "images = sorted((root / \"imagesTr\").glob(\"lung_*.nii.gz\"))\n",
        "labels = sorted((root / \"labelsTr\").glob(\"lung_*.nii.gz\"))\n",
        "\n",
        "# wybierz przyk≈Çadowy plik (np. 2)\n",
        "sample_path = images[2]                       # obraz\n",
        "sample_path_label = labels[2]                 # odpowiadajƒÖca etykieta\n",
        "\n",
        "print(\"Sample image path:\", sample_path)\n",
        "print(\"Sample label path:\", sample_path_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fabulous-toolbox",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:52.911804Z",
          "start_time": "2025-11-06T21:24:52.909203Z"
        },
        "id": "fabulous-toolbox"
      },
      "outputs": [],
      "source": [
        "sample_path, sample_path_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suffering-unknown",
      "metadata": {
        "id": "suffering-unknown"
      },
      "source": [
        "Load NIfTI and extract image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "external-narrative",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:53.622645Z",
          "start_time": "2025-11-06T21:24:52.917467Z"
        },
        "id": "external-narrative"
      },
      "outputs": [],
      "source": [
        "data = nib.load(sample_path)\n",
        "label = nib.load(sample_path_label)\n",
        "\n",
        "ct = data.get_fdata()\n",
        "mask = label.get_fdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appropriate-bidder",
      "metadata": {
        "id": "appropriate-bidder"
      },
      "source": [
        "**Task: Find out the orientation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floating-malawi",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:53.628983Z",
          "start_time": "2025-11-06T21:24:53.626622Z"
        },
        "id": "floating-malawi"
      },
      "outputs": [],
      "source": [
        "nib.aff2axcodes(data.affine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "digital-measurement",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:24:54.014677Z",
          "start_time": "2025-11-06T21:24:53.634651Z"
        },
        "id": "digital-measurement"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "camera = Camera(fig)  # create the camera object from celluloid\n",
        "\n",
        "for i in range(0, ct.shape[2], 2):  # axial view\n",
        "    plt.imshow(ct[:,:,i], cmap=\"bone\")\n",
        "    mask_ = np.ma.masked_where(mask[:,:,i]==0, mask[:,:,i])\n",
        "    plt.imshow(mask_, alpha=0.5, cmap=\"autumn\")\n",
        "    #plt.axis(\"off\")\n",
        "    camera.snap()  # Store the current slice\n",
        "\n",
        "\n",
        "\n",
        "# Tworzenie i zapis animacji\n",
        "animation = camera.animate(interval=100)\n",
        "animation.save(\"/content/ct_mask_animation_raw.mp4\", writer=\"ffmpeg\")\n",
        "\n",
        "# --- üé¨ wy≈õwietl w notebooku ---\n",
        "from IPython.display import Video\n",
        "Video('/content/ct_mask_animation_raw.mp4', embed=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "representative-phoenix",
      "metadata": {
        "id": "representative-phoenix"
      },
      "source": [
        "## Preprocessing\n",
        "**Task: Please perform the following preprocessing steps:**\n",
        "\n",
        "1. CT images have a fixed range from -1000 to 3071. **Thus you could normalize by dividing by 3071** <br /> You don't need to compute mean and standard deviation for this task\n",
        "2. As we want to focus on lung tumors, we can crop away parts of the lower abdomen to reduce the complexity and help the network learn. As an example, **you might skip the first 30 slices (from lower abdomen to the neck)** (last axis)\n",
        "3. As we want to tackle this task on a slice level (2D) and not on a subject level (3D) to reduce the computational cost **you should store the preprocessed data as 2d files**, because reading a single slice is much faster than loading the complete NIfTI file.\n",
        "4. Resize the single slices and masks to (256, 256) (when resizing the mask, pass interpolation=cv2.INTER_NEAREST to the resize function to apply nearest neighbour interpolation)\n",
        "\n",
        "Loop over all_files and apply the preprocessing steps. <br />\n",
        "\n",
        "Additionally, please make sure that all scans have the same orientation\n",
        "\n",
        "In the preprocessing loop, you need to create a directory for each subject containg the ct and label slices with identical names. <br />\n",
        "E.g:\n",
        "* 0/data/0.npy\n",
        "* 0/masks/0.npy\n",
        "\n",
        "Store the last 6 subjects as validation data\n",
        "\n",
        "PS: Feel free to try the lung window!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/Task06_Lung -name \"._*\" -type f -delete"
      ],
      "metadata": {
        "id": "nbdX8myV54nu"
      },
      "id": "nbdX8myV54nu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "root = Path(\"/content/Task06_Lung\")\n",
        "save_root = root / \"Preprocessed\"\n",
        "\n",
        "all_files = sorted((root / \"imagesTr\").glob(\"*.nii.gz\"))\n",
        "\n",
        "for counter, path_to_ct_data in enumerate(tqdm(all_files)):\n",
        "    path_to_label = root / \"labelsTr\" / path_to_ct_data.name\n",
        "\n",
        "    ct_data = nib.load(path_to_ct_data).get_fdata().astype(np.float32)\n",
        "    label_data = nib.load(path_to_label).get_fdata().astype(np.float32)\n",
        "\n",
        "    ct_data = ct_data[:, :, 30:] / 3071.0\n",
        "    label_data = label_data[:, :, 30:]\n",
        "\n",
        "    if counter < len(all_files) - 6:\n",
        "        current_path = save_root / \"train\" / str(counter)\n",
        "    else:\n",
        "        current_path = save_root / \"val\" / str(counter)\n",
        "\n",
        "    for i in range(ct_data.shape[-1]):\n",
        "        slice_ = cv2.resize(ct_data[:, :, i], (256, 256))\n",
        "        mask_ = cv2.resize(label_data[:, :, i], (256, 256), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        (current_path / \"data\").mkdir(parents=True, exist_ok=True)\n",
        "        (current_path / \"masks\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        np.save(current_path / \"data\" / f\"{i}.npy\", slice_)\n",
        "        np.save(current_path / \"masks\" / f\"{i}.npy\", mask_)"
      ],
      "metadata": {
        "id": "zJcNgjxPpvy3"
      },
      "id": "zJcNgjxPpvy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "gentle-clinic",
      "metadata": {
        "id": "gentle-clinic"
      },
      "source": [
        "## Validate preprocessed data\n",
        "**Task: Take a look at your stored files and inspect if everything worked as expected**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fifteen-failing",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:26:17.920250Z",
          "start_time": "2025-11-06T21:21:27.735707Z"
        },
        "id": "fifteen-failing"
      },
      "outputs": [],
      "source": [
        "path = Path(\"Task06_Lung/Preprocessed/train/2\")  # Select a subject. Check the folder if it exists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applicable-handbook",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:26:17.930715Z",
          "start_time": "2025-11-06T21:23:43.559766Z"
        },
        "id": "applicable-handbook"
      },
      "outputs": [],
      "source": [
        "\n",
        "list(path.glob(\"*\"))\n",
        "\n",
        "# Choose a file and load slice + mask\n",
        "file = \"120.npy\"\n",
        "slice = np.load(path/\"data\"/file)\n",
        "mask = np.load(path/\"masks\"/file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "base_path = Path(\"/content/Task06_Lung/Preprocessed/train\")\n",
        "\n",
        "max_pixels = 0\n",
        "best_subject = None\n",
        "best_slice_idx = None\n",
        "\n",
        "for subj in sorted(base_path.glob(\"*\")):\n",
        "    for mask_file in (subj / \"masks\").glob(\"*.npy\"):\n",
        "        mask = np.load(mask_file)\n",
        "        tumor_pixels = np.sum(mask > 0)\n",
        "        if tumor_pixels > max_pixels:\n",
        "            max_pixels = tumor_pixels\n",
        "            best_subject = subj.name\n",
        "            best_slice_idx = mask_file.stem  # npy filename without .npy\n",
        "\n",
        "print(f\"üìç Najwiƒôkszy guz w: {best_subject}/slice {best_slice_idx}\")\n",
        "print(f\"ü©∏ Liczba pikseli guza: {max_pixels}\")\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "\n",
        "subject = best_subject\n",
        "slice_idx = best_slice_idx\n",
        "\n",
        "# --- wczytaj dane ---\n",
        "slice_ = np.load(f\"/content/Task06_Lung/Preprocessed/train/{subject}/data/{slice_idx}.npy\")\n",
        "mask = np.load(f\"/content/Task06_Lung/Preprocessed/train/{subject}/masks/{slice_idx}.npy\")\n",
        "\n",
        "# --- normalizacja i RGB overlay ---\n",
        "slice_norm = (slice_ - np.min(slice_)) / (np.max(slice_) - np.min(slice_) + 1e-8)\n",
        "slice_uint8 = (slice_norm * 255).astype(np.uint8)\n",
        "mask_uint8 = (mask * 255).astype(np.uint8)\n",
        "\n",
        "rgb = np.stack([slice_uint8]*3, axis=-1)\n",
        "overlay = rgb.copy()\n",
        "overlay[mask_uint8 > 0, 0] = 255  # Red\n",
        "overlay[mask_uint8 > 0, 1] = 0\n",
        "overlay[mask_uint8 > 0, 2] = 0\n",
        "\n",
        "# --- wizualizacja ---\n",
        "fig, axis = plt.subplots(1, 2, figsize=(8, 8))\n",
        "axis[0].imshow(slice_uint8, cmap=\"gray\")\n",
        "axis[0].set_title(f\"CT slice {slice_idx}\")\n",
        "\n",
        "axis[1].imshow(overlay)\n",
        "axis[1].set_title(\"Overlay (max tumor area)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "save_path = f\"/content/ct_mask_largest_overlay.png\"\n",
        "plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "display(Image(filename=save_path))"
      ],
      "metadata": {
        "id": "FlW9Mu0mrKDd"
      },
      "id": "FlW9Mu0mrKDd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"slice shape:\", slice.shape)\n",
        "print(\"mask shape:\", mask.shape)\n",
        "print(\"unique mask values:\", np.unique(mask))\n",
        "print(slice.min(), slice.max())"
      ],
      "metadata": {
        "id": "QeIxJpMBoyuC"
      },
      "id": "QeIxJpMBoyuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "laughing-palace",
      "metadata": {
        "id": "laughing-palace"
      },
      "source": [
        "## Introduction\n",
        "In this notebook we will create the dataset class used to feed slices and corresponding segmentation masks to the network during training.\n",
        "It is identical to the CardiacDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c9aa9568edf5af",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:39:30.367168Z",
          "start_time": "2025-11-06T21:39:30.362622Z"
        },
        "id": "52c9aa9568edf5af"
      },
      "outputs": [],
      "source": [
        "# --- Fix for NumPy 2.0 removal of np.sctypes ---\n",
        "import numpy as np\n",
        "\n",
        "if not hasattr(np, \"sctypes\"):\n",
        "    np.sctypes = {\n",
        "        'int': [np.int8, np.int16, np.int32, np.int64],\n",
        "        'uint': [np.uint8, np.uint16, np.uint32, np.uint64],\n",
        "        'float': [np.float16, np.float32, np.float64],\n",
        "        'complex': [np.complex64, np.complex128],\n",
        "        'others': [bool, object, str, bytes],\n",
        "    }\n",
        "# ------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dependent-covering",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:40:13.553817Z",
          "start_time": "2025-11-06T21:39:33.850331Z"
        },
        "jupyter": {
          "is_executing": true
        },
        "id": "dependent-covering"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import imgaug\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coated-tractor",
      "metadata": {
        "id": "coated-tractor"
      },
      "source": [
        "## DataSet Creation\n",
        "We need to implement the following functionality:\n",
        "1. Create a list of all 2D slices. To so we need to extract all slices from all subjects\n",
        "2. Extract the corresponding label path for each slice path\n",
        "3. Load slice and label\n",
        "4. Data Augmentation. Make sure that slice and mask are augmented identically. imgaug handles this for us, thus we will not use torchvision.transforms for that\n",
        "5. Return slice and mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "driven-replacement",
      "metadata": {
        "id": "driven-replacement"
      },
      "outputs": [],
      "source": [
        "class LungDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, augment_params):\n",
        "        self.all_files = self.extract_files(root)\n",
        "        self.augment_params = augment_params\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_files(root):\n",
        "        \"\"\"\n",
        "        Extract the paths to all slices given the root path (ends with train or val)\n",
        "        \"\"\"\n",
        "        files = []\n",
        "        for subject in root.glob(\"*\"):   # Iterate over the subjects\n",
        "            slice_path = subject/\"data\"  # Get the slices for current subject\n",
        "            for slice in slice_path.glob(\"*\"):\n",
        "                files.append(slice)\n",
        "        return files\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def change_img_to_label_path(path):\n",
        "        \"\"\"\n",
        "        Replace data with mask to get the masks\n",
        "        \"\"\"\n",
        "        parts = list(path.parts)\n",
        "        parts[parts.index(\"data\")] = \"masks\"\n",
        "        return Path(*parts)\n",
        "\n",
        "    def augment(self, slice, mask):\n",
        "        \"\"\"\n",
        "        Augments slice and segmentation mask in the exact same way\n",
        "        Note the manual seed initialization\n",
        "        \"\"\"\n",
        "        ###################IMPORTANT###################\n",
        "        # Fix for https://discuss.pytorch.org/t/dataloader-workers-generate-the-same-random-augmentations/28830/2\n",
        "        random_seed = torch.randint(0, 1000000, (1,))[0].item()\n",
        "        imgaug.seed(random_seed)\n",
        "        #####################################################\n",
        "\n",
        "        # mask = SegmentationMapsOnImage(mask, mask.shape) wazna poprawka na dole\n",
        "        mask = SegmentationMapsOnImage(mask.astype(np.uint8), mask.shape)\n",
        "\n",
        "        slice_aug, mask_aug = self.augment_params(image=slice, segmentation_maps=mask)\n",
        "        mask_aug = mask_aug.get_arr()\n",
        "        return slice_aug, mask_aug\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset (length of all files)\n",
        "        \"\"\"\n",
        "        return len(self.all_files)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Given an index return the (augmented) slice and corresponding mask\n",
        "        Add another dimension for pytorch\n",
        "        \"\"\"\n",
        "        file_path = self.all_files[idx]\n",
        "        mask_path = self.change_img_to_label_path(file_path)\n",
        "        slice = np.load(file_path)\n",
        "        mask = np.load(mask_path)\n",
        "\n",
        "        if self.augment_params:\n",
        "            slice, mask = self.augment(slice, mask)\n",
        "\n",
        "        return np.expand_dims(slice, 0), np.expand_dims(mask, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scientific-wallpaper",
      "metadata": {
        "id": "scientific-wallpaper"
      },
      "source": [
        "Now we can test our dataset!\n",
        "Let us at first define the data augmentation routine corresponding of scaling and rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-march",
      "metadata": {
        "id": "solved-march"
      },
      "outputs": [],
      "source": [
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(scale=(0.85, 1.15), # zoom in or out\n",
        "               rotate=(-45, 45)),  # rotate up to 45 degrees\n",
        "    iaa.ElasticTransformation()\n",
        "                ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sound-gothic",
      "metadata": {
        "id": "sound-gothic"
      },
      "outputs": [],
      "source": [
        "# Create the dataset object\n",
        "path = Path(\"Task06_Lung/Preprocessed/train/\")\n",
        "dataset = LungDataset(path, seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "operating-ensemble",
      "metadata": {
        "id": "operating-ensemble"
      },
      "source": [
        "Finally we can visualize our dataset.\n",
        "To make sure that the augmentation works we access the same element multiple times and check whether the segmentation masks fit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# --- Rysuj 3x3 przyk≈Çady augmentacji ---\n",
        "fig, axis = plt.subplots(3, 3, figsize=(9, 9))\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        slice, mask = dataset[19]\n",
        "        mask_ = np.ma.masked_where(mask == 0, mask)\n",
        "\n",
        "        axis[i][j].imshow(slice[0], cmap=\"bone\")\n",
        "        axis[i][j].imshow(mask_[0], cmap=\"autumn\", alpha=0.5)\n",
        "        axis[i][j].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Sample augmentations\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- üíæ Zapisz do pliku PNG ---\n",
        "save_path = \"/content/sample_augmentations.png\"\n",
        "plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "# --- üñºÔ∏è Wy≈õwietl zapisany obraz ---\n",
        "display(Image(filename=save_path))"
      ],
      "metadata": {
        "id": "-zmdGChJ71lS"
      },
      "id": "-zmdGChJ71lS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "victorian-affair",
      "metadata": {
        "id": "victorian-affair"
      },
      "source": [
        "Nice!\n",
        "With above dataset we can finally create the model and train the Lung Cancer Segmentation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "horizontal-player",
      "metadata": {
        "id": "horizontal-player"
      },
      "source": [
        "## Introduction\n",
        "In this notebook we will create the model for the Tumor Segmentation! <br />\n",
        "Nothing is changed compared to the Atrium Segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "noticed-richardson",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:38:09.026912Z",
          "start_time": "2025-11-06T21:38:08.633886Z"
        },
        "id": "noticed-richardson"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "innocent-rainbow",
      "metadata": {
        "id": "innocent-rainbow"
      },
      "source": [
        "## UNET\n",
        "The idea behind a UNET is that we have \"Downconvolutions\" which are reducing the size of the image combined with increasing filter size followed by \"Upconvolutions\" which increase the image size up to the original size while reducing the number of filters. <br />\n",
        "All pairs between Up- and Downconvolutions are linked with skip connections.<br />\n",
        "Upsampling can either be done by interpolation or by UpConvolutions (ConvTranspose2d)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alternative-grove",
      "metadata": {
        "id": "alternative-grove"
      },
      "source": [
        "## Convolutions\n",
        "At first we write the class, repsonsible for the convolutions.\n",
        "We will use two convolutions between each down- or upconvolution step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "going-nicholas",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:38:13.573042Z",
          "start_time": "2025-11-06T21:38:13.570036Z"
        },
        "id": "going-nicholas"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Helper Class which implements the intermediate Convolutions\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.step = torch.nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "                                        torch.nn.ReLU(),\n",
        "                                        torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "                                        torch.nn.ReLU())\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.step(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "turned-campus",
      "metadata": {
        "id": "turned-campus"
      },
      "source": [
        "## UNET\n",
        "With the help of DoubleConv we can easily implement the UNET by combining **DoubleConv** with **maxpooling** for  DownConvolutions or **DoubleConv** with **Upsample** for UpConv.\n",
        "\n",
        "Feel free to replace Upsample with ConvTranspose2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blocked-killer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:38:19.621880Z",
          "start_time": "2025-11-06T21:38:19.614594Z"
        },
        "id": "blocked-killer"
      },
      "outputs": [],
      "source": [
        "class UNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a UNet for the Segmentation\n",
        "    We use 3 down- and 3 UpConvolutions and two Convolutions in each step\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Sets up the U-Net Structure\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        ############# DOWN #####################\n",
        "        self.layer1 = DoubleConv(1, 64)  # MRI -> One channel\n",
        "        self.layer2 = DoubleConv(64, 128)\n",
        "        self.layer3 = DoubleConv(128, 256)\n",
        "        self.layer4 = DoubleConv(256, 512)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        ############## UP #######################\n",
        "        self.layer5 = DoubleConv(512 + 256, 256)\n",
        "        self.layer6 = DoubleConv(256+128, 128)\n",
        "        self.layer7 = DoubleConv(128+64, 64)\n",
        "        self.layer8 = torch.nn.Conv2d(64, 1, 1)  # Binary label -> One class\n",
        "        #########################################\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ####### DownConv 1#########\n",
        "        x1 = self.layer1(x)\n",
        "        x1m = self.maxpool(x1)\n",
        "        ###########################\n",
        "\n",
        "        ####### DownConv 2#########\n",
        "        x2 = self.layer2(x1m)\n",
        "        x2m = self.maxpool(x2)\n",
        "        ###########################\n",
        "\n",
        "        ####### DownConv 3#########\n",
        "        x3 = self.layer3(x2m)\n",
        "        x3m = self.maxpool(x3)\n",
        "        ###########################\n",
        "\n",
        "        ##### Intermediate Layer ##\n",
        "        x4 = self.layer4(x3m)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 1#########\n",
        "        x5 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x4)  # Upsample with a factor of 2\n",
        "        #x5 = torch.nn.ConvTranspose2d(512, 512, 2, 2)(x4)\n",
        "        x5 = torch.cat([x5, x3], dim=1)  # Skip-Connection\n",
        "        x5 = self.layer5(x5)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 2#########\n",
        "        x6 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x5)\n",
        "        #x6 = torch.nn.ConvTranspose2d(256, 256, 2, 2)(x5)\n",
        "        x6 = torch.cat([x6, x2], dim=1)  # Skip-Connection\n",
        "        x6 = self.layer6(x6)\n",
        "        ###########################\n",
        "\n",
        "        ####### UpCONV 3#########\n",
        "        x7 = torch.nn.Upsample(scale_factor=2, mode=\"bilinear\")(x6)\n",
        "        #x7 = torch.nn.ConvTranspose2d(128, 128, 2, 2)(x6)\n",
        "        x7 = torch.cat([x7, x1], dim=1)\n",
        "        x7 = self.layer7(x7)\n",
        "        ###########################\n",
        "\n",
        "        ####### Predicted segmentation#########\n",
        "        ret = self.layer8(x7)\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "removable-plumbing",
      "metadata": {
        "id": "removable-plumbing"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ongoing-cattle",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:38:23.941958Z",
          "start_time": "2025-11-06T21:38:23.912495Z"
        },
        "id": "ongoing-cattle"
      },
      "outputs": [],
      "source": [
        "model = UNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grand-samuel",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-11-06T21:38:25.466968Z",
          "start_time": "2025-11-06T21:38:25.150892Z"
        },
        "id": "grand-samuel"
      },
      "outputs": [],
      "source": [
        "random_input = torch.randn(1, 1, 256, 256)\n",
        "output = model(random_input)\n",
        "assert output.shape == torch.Size([1, 1, 256, 256])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "corresponding-replication",
      "metadata": {
        "id": "corresponding-replication"
      },
      "source": [
        "## Introduction\n",
        "Finally we are going to train our tumor segmentation network. <br />\n",
        "Here we apply some small changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pregnant-density",
      "metadata": {
        "id": "pregnant-density"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import imgaug.augmenters as iaa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from celluloid import Camera\n",
        "\n",
        "#from dataset import LungDataset\n",
        "#from model import UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brutal-implementation",
      "metadata": {
        "id": "brutal-implementation"
      },
      "source": [
        "## Dataset Creation\n",
        "Here we create the train and validation dataset. <br />\n",
        "Additionally we define our data augmentation pipeline.\n",
        "Subsequently the two dataloaders are created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broadband-arcade",
      "metadata": {
        "id": "broadband-arcade"
      },
      "outputs": [],
      "source": [
        "seq = iaa.Sequential([\n",
        "    iaa.Affine(translate_percent=(0.15),\n",
        "               scale=(0.85, 1.15), # zoom in or out\n",
        "               rotate=(-45, 45)#\n",
        "               ),  # rotate up to 45 degrees\n",
        "    iaa.ElasticTransformation()  # Elastic Transformations\n",
        "                ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "laughing-rehabilitation",
      "metadata": {
        "id": "laughing-rehabilitation"
      },
      "outputs": [],
      "source": [
        "# Create the dataset objects\n",
        "train_path = Path(\"Task06_Lung/Preprocessed/train/\")\n",
        "val_path = Path(\"Task06_Lung/Preprocessed/val/\")\n",
        "\n",
        "train_dataset = LungDataset(train_path, seq)\n",
        "val_dataset = LungDataset(val_path, None)\n",
        "\n",
        "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rubber-theater",
      "metadata": {
        "id": "rubber-theater"
      },
      "source": [
        "## Oversampling to tackle strong class imbalance\n",
        "Lung tumors are often very small, thus we need to make sure that our model does not learn a trivial solution which simply outputs 0 for all voxels.<br />\n",
        "In this notebook we will use oversampling to sample slices which contain a tumor more often.\n",
        "\n",
        "To do so we can use the **WeightedRandomSampler** provided by pytorch which needs a weight for each sample in the dataset.\n",
        "Typically you have one weight for each class, which means that we need to calculate two weights, one for slices without tumors and one for slices with a tumor and create list that assigns each sample from the dataset the corresponding weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amino-nashville",
      "metadata": {
        "id": "amino-nashville"
      },
      "source": [
        "To do so, we at first need to create a list containing only the class labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comic-fence",
      "metadata": {
        "id": "comic-fence"
      },
      "outputs": [],
      "source": [
        "target_list = []\n",
        "for _, label in tqdm(train_dataset):\n",
        "    # Check if mask contains a tumorous pixel:\n",
        "    if np.any(label):\n",
        "        target_list.append(1)\n",
        "    else:\n",
        "        target_list.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_array = np.array(target_list)\n",
        "print(f\"üîç ≈ÅƒÖcznie: {len(target_array)} slice‚Äô√≥w\")\n",
        "print(f\"üß¨ Z guzem: {np.sum(target_array)}\")\n",
        "print(f\"‚ö™ Bez guza: {len(target_array) - np.sum(target_array)}\")\n",
        "print(f\"üìä Procent z guzem: {100*np.mean(target_array):.2f}%\")"
      ],
      "metadata": {
        "id": "9HJtMkWy8q6S"
      },
      "id": "9HJtMkWy8q6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "commercial-antenna",
      "metadata": {
        "id": "commercial-antenna"
      },
      "source": [
        "Then we can calculate the weight for each class:\n",
        "To do so, we can simply compute the fraction between the classes and then create the weight list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-connectivity",
      "metadata": {
        "id": "thirty-connectivity"
      },
      "outputs": [],
      "source": [
        "uniques = np.unique(target_list, return_counts=True)\n",
        "uniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bridal-advantage",
      "metadata": {
        "id": "bridal-advantage"
      },
      "outputs": [],
      "source": [
        "fraction = uniques[1][0] / uniques[1][1]\n",
        "fraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laden-brunswick",
      "metadata": {
        "id": "laden-brunswick"
      },
      "source": [
        "Subsequently we assign the weight 1 to each slice without a tumor and ~9 to each slice with a tumor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "planned-stanford",
      "metadata": {
        "id": "planned-stanford"
      },
      "outputs": [],
      "source": [
        "weight_list = []\n",
        "for target in target_list:\n",
        "    if target == 0:\n",
        "        weight_list.append(1)\n",
        "    else:\n",
        "        weight_list.append(fraction)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entire-tampa",
      "metadata": {
        "id": "entire-tampa"
      },
      "source": [
        "Finally we create the sampler which we can pass to the DataLoader.\n",
        "**Important:** Only use a sampler for the train loader! We don't want to change the validation data to get a real validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "backed-acoustic",
      "metadata": {
        "id": "backed-acoustic"
      },
      "outputs": [],
      "source": [
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight_list, len(weight_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-assets",
      "metadata": {
        "id": "final-assets"
      },
      "outputs": [],
      "source": [
        "batch_size = 16#TODO\n",
        "num_workers = 6# TODO\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, sampler=sampler)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spread-growth",
      "metadata": {
        "id": "spread-growth"
      },
      "source": [
        "We can verify that our sampler works by taking a batch from the train loader and count how many labels are larger than zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unauthorized-wesley",
      "metadata": {
        "id": "unauthorized-wesley"
      },
      "outputs": [],
      "source": [
        "verify_sampler = next(iter(train_loader))  # Take one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bright-glasgow",
      "metadata": {
        "id": "bright-glasgow"
      },
      "outputs": [],
      "source": [
        "(verify_sampler[1][:,0]).sum([1, 2]) > 0  # ~ half the batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mechanical-startup",
      "metadata": {
        "id": "mechanical-startup"
      },
      "source": [
        "## Loss\n",
        "\n",
        "As this is a harder task to train you might try different loss functions:\n",
        "We achieved best results by using the Binary Cross Entropy instead of the Dice Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prime-radio",
      "metadata": {
        "id": "prime-radio"
      },
      "source": [
        "## Full Segmentation Model\n",
        "\n",
        "We now combine everything into the full pytorch lightning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "little-dover",
      "metadata": {
        "id": "little-dover"
      },
      "outputs": [],
      "source": [
        "class TumorSegmentation(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = UNet()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
        "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, data):\n",
        "        pred = self.model(data)\n",
        "        return pred\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        ct, mask = batch\n",
        "        mask = mask.float()\n",
        "\n",
        "        pred = self(ct)\n",
        "        loss = self.loss_fn(pred, mask)\n",
        "\n",
        "        # Logs\n",
        "        self.log(\"Train_Dice\", loss)\n",
        "        if batch_idx % 50 == 0:\n",
        "            self.log_images(ct.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        ct, mask = batch\n",
        "        mask = mask.float()\n",
        "\n",
        "        pred = self(ct)\n",
        "        loss = self.loss_fn(pred, mask)\n",
        "\n",
        "        # Logs\n",
        "        self.log(\"Val_Dice\", loss)\n",
        "        if batch_idx % 50 == 0:\n",
        "            self.log_images(ct.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def log_images(self, ct, pred, mask, name):\n",
        "\n",
        "        results = []\n",
        "\n",
        "        pred = pred > 0.5 # As we use the sigomid activation function, we threshold at 0.5\n",
        "\n",
        "\n",
        "        fig, axis = plt.subplots(1, 2)\n",
        "        axis[0].imshow(ct[0][0], cmap=\"bone\")\n",
        "        mask_ = np.ma.masked_where(mask[0][0]==0, mask[0][0])\n",
        "        axis[0].imshow(mask_, alpha=0.6)\n",
        "        axis[0].set_title(\"Ground Truth\")\n",
        "\n",
        "        axis[1].imshow(ct[0][0], cmap=\"bone\")\n",
        "        mask_ = np.ma.masked_where(pred[0][0]==0, pred[0][0])\n",
        "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
        "        axis[1].set_title(\"Pred\")\n",
        "\n",
        "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
        "\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
        "        return [self.optimizer]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "featured-nature",
      "metadata": {
        "id": "featured-nature"
      },
      "outputs": [],
      "source": [
        "# Instanciate the model\n",
        "model = TumorSegmentation()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- CALLBACK: zapisywanie najlepszych modeli ---\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"./checkpoints\",         # katalog na checkpointy\n",
        "    filename=\"epoch{epoch:02d}-valdice{Val_Dice:.4f}\",  # nazwa pliku z metrykƒÖ\n",
        "    monitor=\"Val_Dice\",              # üëà musi byƒá taka sama jak nazwa w self.log()\n",
        "    save_top_k=3,                    # np. tylko 3 najlepsze checkpointy\n",
        "    mode=\"min\",                      # je≈õli loss/Dice ‚Äî zwykle \"min\"; je≈õli accuracy ‚Äî \"max\"\n",
        "    save_last=True,                  # zapisuj te≈º ostatni model\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --- LOGGER: TensorBoard ---\n",
        "logger = TensorBoardLogger(save_dir=\"./logs\", name=\"lung_unet\")\n",
        "\n",
        "# --- TRENER: GPU, logowanie, callbacki ---\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",     # u≈ºyj GPU (Colab T4)\n",
        "    devices=1,             # tylko jeden GPU\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    max_epochs=20,\n",
        "    log_every_n_steps=10,  # loguj co 10 batchy (nie co batch)\n",
        ")"
      ],
      "metadata": {
        "id": "eFZRnmcEzgVO"
      },
      "id": "eFZRnmcEzgVO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handy-testing",
      "metadata": {
        "scrolled": true,
        "id": "handy-testing"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "metadata": {
        "id": "illegal-harvest"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n"
      ],
      "id": "illegal-harvest"
    },
    {
      "metadata": {
        "id": "b08556ba43060bf3"
      },
      "cell_type": "markdown",
      "source": [
        "# Dice Score ‚Äî Evaluation Metric for Medical Image Segmentation\n",
        "\n",
        "The **Dice Score** (also called *Dice Coefficient* or *S√∏rensen‚ÄìDice index*)\n",
        "is one of the most common evaluation metrics in **medical image segmentation**,\n",
        "especially for CT or MRI scans where the model (e.g. **U-Net**) detects tumors or organs.\n",
        "\n",
        "---\n",
        "\n",
        "### Mathematical Definition\n",
        "\n",
        "For two binary masks ‚Äî prediction \\( P \\) and ground truth \\( G \\):\n",
        "\n",
        "$$\n",
        "\\text{Dice}(P, G) = \\frac{2 |P \\cap G|}{|P| + |G|}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ |P \\cap G| $ ‚Äì number of pixels correctly predicted as tumor,\n",
        "- $ |P| $ ‚Äì number of pixels predicted as tumor,\n",
        "- $ |G| $ ‚Äì number of pixels actually belonging to tumor.\n",
        "\n",
        "---\n",
        "\n",
        "###  Practical Meaning\n",
        "\n",
        "| Dice Score | Interpretation |\n",
        "|-------------|----------------|\n",
        "| **1.0** | Perfect overlap (model segmentation = ground truth) |\n",
        "| **0.0** | No overlap (model completely wrong) |\n",
        "| **0.5‚Äì0.9** | Partial overlap ‚Äî typical for real segmentation tasks |\n",
        "\n",
        "> üí° Dice Score is very sensitive to small regions ‚Äî\n",
        "> if tumors are tiny, even a few wrong pixels can drastically reduce the score.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### How to compute Dice Score\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Compute overall Dice Score\n",
        "dice_score = DiceScore()(\n",
        "    torch.from_numpy(preds),\n",
        "    torch.from_numpy(labels).unsqueeze(0).float()\n",
        ")\n",
        "print(f\"The Val Dice Score is: {dice_score:.4f}\")\n",
        "```\n",
        "---\n"
      ],
      "id": "b08556ba43060bf3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "challenging-color",
      "metadata": {
        "id": "challenging-color"
      },
      "outputs": [],
      "source": [
        "class DiceScore(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    class to compute the Dice Loss\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, mask):\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        pred = torch.flatten(pred)\n",
        "        mask = torch.flatten(mask)\n",
        "\n",
        "        counter = (pred * mask).sum()  # Counter\n",
        "        denum = pred.sum() + mask.sum()  # denominator\n",
        "        dice = (2*counter)/denum\n",
        "\n",
        "        return dice\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onedrivedownloader import download\n",
        "url = \"https://ujchmura-my.sharepoint.com/:u:/g/personal/marcin_sztukowski_student_uj_edu_pl/EeIxSRDsbaROj51n9jl7TDoBTifXVohnxCVymwhyiGrBSQ?e=IRznML\"\n",
        "download(url, filename=\"epoch=29-step=53759.ckpt\", unzip=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VRoQV4aDln4B"
      },
      "id": "VRoQV4aDln4B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "digital-fourth",
      "metadata": {
        "id": "digital-fourth"
      },
      "source": [
        "Compute overall Dice Score: This is not a bad result!\n",
        "Those tumors are extremely small and already some wrongly segmented pixels strongly reduce the Dice Score.\n",
        "The Visualization below demonstrates that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breathing-oregon",
      "metadata": {
        "id": "breathing-oregon"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = TumorSegmentation.load_from_checkpoint(\"/content/epoch=29-step=53759.ckpt\")\n",
        "model.eval();\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device);\n",
        "\n",
        "\n",
        "preds = []\n",
        "labels = []\n",
        "\n",
        "for slice, label in val_dataset:\n",
        "    slice = torch.tensor(slice).float().to(device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        pred = torch.sigmoid(model(slice))\n",
        "    preds.append(pred.cpu().numpy())\n",
        "    labels.append(label)\n",
        "\n",
        "preds = np.array(preds)\n",
        "labels = np.array(labels)\n",
        "\n",
        "\n",
        "dice_score = DiceScore()(torch.from_numpy(preds), torch.from_numpy(labels).unsqueeze(0).float())\n",
        "print(f\"The Val Dice Score is: {dice_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from celluloid import Camera\n",
        "from IPython.display import HTML, Video\n",
        "import numpy as np\n",
        "import warnings\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import os\n",
        "\n",
        "\n",
        "def load_trained_model(checkpoint_path, model_class = TumorSegmentation, device=None):\n",
        "    \"\"\"\n",
        "    Wczytuje wytrenowany model PyTorch Lightning z pliku .ckpt\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: ≈õcie≈ºka do pliku checkpoint (.ckpt)\n",
        "        model_class: klasa modelu (np. TumorSegmentation)\n",
        "        device: 'cuda', 'cpu' lub None (automatycznie wykrywane)\n",
        "\n",
        "    Returns:\n",
        "        model: za≈Çadowany i gotowy do ewaluacji model\n",
        "    \"\"\"\n",
        "\n",
        "    # automatyczne wykrycie urzƒÖdzenia\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # wczytanie modelu z checkpointu\n",
        "    print(f\"üîÑ Loading model from: {checkpoint_path}\")\n",
        "    model = model_class.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "    # przeniesienie na GPU/CPU\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úÖ Model loaded successfully on {device}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_segmentation_video(scan, segmentation, save_path=\"/content/ct_tumor_animation.mp4\",\n",
        "                              step=2, fps=10):\n",
        "    \"\"\"\n",
        "    Tworzy wideo MP4 z segmentacji CT bez u≈ºycia matplotlib.animation (dzia≈Ça w Colabie bez b≈Çƒôd√≥w).\n",
        "    \"\"\"\n",
        "    os.makedirs(\"/content/tmp_frames\", exist_ok=True)\n",
        "    tmp_dir = \"/content/tmp_frames\"\n",
        "\n",
        "    # --- zapis pojedynczych klatek ---\n",
        "    for idx, i in enumerate(tqdm(range(0, len(scan), step), desc=\"Rendering frames\")):\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(scan[i], cmap=\"bone\")\n",
        "        mask = np.ma.masked_where(segmentation[i] == 0, segmentation[i])\n",
        "        plt.imshow(mask, alpha=0.5, cmap=\"autumn\")\n",
        "        plt.axis(\"off\")\n",
        "        frame_path = f\"{tmp_dir}/frame_{idx:04d}.png\"\n",
        "        plt.savefig(frame_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "    # --- sk≈Çadanie filmu przez ffmpeg ---\n",
        "    print(\"üíæ Rendering wideo przez ffmpeg...\")\n",
        "    cmd = f\"ffmpeg -y -framerate {fps} -i {tmp_dir}/frame_%04d.png -c:v libx264 -pix_fmt yuv420p {save_path}\"\n",
        "    os.system(cmd)\n",
        "\n",
        "    # --- czyszczenie ---\n",
        "    for f in os.listdir(tmp_dir):\n",
        "        os.remove(os.path.join(tmp_dir, f))\n",
        "    os.rmdir(tmp_dir)\n",
        "\n",
        "    print(f\"‚úÖ Zapisano: {save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_single_patient(model, ct_volume, device=\"cuda\", threshold=0.5):\n",
        "    \"\"\"\n",
        "    Segmentacja jednego pacjenta (CT volume) przy u≈ºyciu modelu (np. U-Net).\n",
        "\n",
        "    Args:\n",
        "        model: wytrenowany model PyTorch (np. U-Net)\n",
        "        ct_volume: numpy array 3D (H, W, N) ‚Äî pe≈Çny tomogram pacjenta\n",
        "        device: 'cuda' lub 'cpu'\n",
        "        threshold: pr√≥g binarny dla maski (np. 0.5)\n",
        "\n",
        "    Returns:\n",
        "        scan: lista 2D obraz√≥w (oryginalnych slice'√≥w)\n",
        "        segmentation: lista 2D masek binarnych\n",
        "        preds: lista masek z warto≈õciami sigmoid (float)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    scan = []\n",
        "    segmentation = []\n",
        "    preds = []\n",
        "\n",
        "    for i in tqdm(range(ct_volume.shape[-1])):\n",
        "        # --- przygotowanie slice‚Äôa ---\n",
        "        slice_ = cv2.resize(ct_volume[:, :, i], (256, 256))\n",
        "        scan.append(slice_)\n",
        "\n",
        "        input_t = torch.tensor(slice_).unsqueeze(0).unsqueeze(0).float().to(device)\n",
        "\n",
        "        # --- predykcja modelu ---\n",
        "        with torch.no_grad():\n",
        "            pred = torch.sigmoid(model(input_t))[0, 0].cpu().numpy()\n",
        "\n",
        "        preds.append(pred)\n",
        "        segmentation.append((pred > threshold).astype(np.uint8))\n",
        "\n",
        "    scan = np.array(scan)\n",
        "    segmentation = np.array(segmentation)\n",
        "    preds = np.array(preds)\n",
        "\n",
        "    return scan, segmentation, preds\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M7lSCF8GyMRs"
      },
      "id": "M7lSCF8GyMRs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Zadanie obowiazkowe : Odpalenie modelu i wizualizacja dzialania modelu\n",
        "\n",
        "W tej czƒô≈õci student samodzielnie **u≈ºyje gotowego, wytrenowanego modelu U-Net** do przeprowadzenia **segmentacji tomografii komputerowej p≈Çuc (CT)**.  \n",
        "Celem jest:\n",
        "\n",
        "1. Wczytanie modelu z pliku `.ckpt`,\n",
        "2. Wczytanie danych 3D konkretnego pacjenta (`.nii.gz`),\n",
        "3. Przeprowadzenie predykcji maski nowotworu dla ka≈ºdej warstwy CT,\n",
        "4. Stworzenie animacji pokazujƒÖcej segmentacjƒô,\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FK0iQissJfnA"
      },
      "id": "FK0iQissJfnA"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# üë©‚Äçüéì CZƒò≈öƒÜ STUDENTA\n",
        "# ======================================================\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from celluloid import Camera\n",
        "from IPython.display import Video\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "MAX_VAL = 3071\n",
        "CROP_SIZE = 30\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#stale do stworzenia wideo\n",
        "STEPS = 2\n",
        "FPS=10\n",
        "\n",
        "\n",
        "# wczytaj pobrany model / checkpoint\n",
        "checkpoint = \"/content/POBRANY_MODEL...\" # TODO\n",
        "model = load_trained_model( ) #TODO\n",
        "subject = Path(\"/content/Task06_Lung/imagesTs/DANE_PACJENTA.nii.gz\") #TODO\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Uzyj nib.load oraz get_fdata aby wczytac dane dowolnego pacjenta\n",
        "# pamietaj aby podzielic przez MAX_VAL wczytane dane aby ustandaryzowac dane\n",
        "# pamietaj aby zrobic CROP wczytanych danych\n",
        "\n",
        "subject = Path(\"/content/Task06_Lung/imagesTs/WYBIERZ_ZDJECIE\") #TODO\n",
        "ct = nib.load  #TODO load\n",
        "ct = ct[...]  # TODO crop\n",
        "scan, segmentation, preds =  ...  #TODO uzyj funkcji wyzej\n",
        "\n",
        "#uzyj gotowej funkcji do stworzenia video z klatek   #TODO\n",
        "Video(\"/content/VIDEO.mp4\", embed=True) #TODO\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HvOew2PSBY-q"
      },
      "id": "HvOew2PSBY-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# üë®‚Äçüè´ CZƒò≈öƒÜ NAUCZYCIELA\n",
        "# ======================================================\n",
        "\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "MAX_VAL = 3071\n",
        "CROP_SIZE = 30\n",
        "#stale do stworzenia wideo\n",
        "STEPS = 2\n",
        "FPS=10\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "checkpoint = \"/content/epoch=29-step=53759.ckpt\"\n",
        "model = load_trained_model(checkpoint_path=checkpoint, model_class=TumorSegmentation)\n",
        "subject = Path(\"/content/Task06_Lung/imagesTs/lung_035.nii.gz\")\n",
        "ct = nib.load(subject).get_fdata() / MAX_VAL  # standardize\n",
        "ct = ct[:,:,CROP_SIZE:]  # crop\n",
        "scan, segmentation, preds = predict_single_patient(model, ct, device=DEVICE)\n",
        "create_segmentation_video(scan, segmentation, \"/content/ct_tumor_animation_35.mp4\", step=STEPS, fps=FPS)\n",
        "Video(\"/content/ct_tumor_animation_35.mp4\", embed=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eHiy8_AS0gIR"
      },
      "id": "eHiy8_AS0gIR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}